<!DOCTYPE html>
<html>

  <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatibale" content="IE=edge">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <title>
          Andy_Lee's home page:Paper Presentation(1)-ImageNet CLassification with Deep Convolutional Neural Networks
      </title>

      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, intial-scale=1">

      <link href="http://fonts.googleapis.com/css?family=Noto Sans::400,700,400italic,700italic" rel="stylesheet" type="text/css" />
      <link href="/css/bootstrap.min.css" rel="stylesheet">
      <link href="/css/abll.css" rel="stylesheet">
      <link href="/css/pygments.css" rel="stylesheet">

      
      <meta name="og:type" content="article">
      <meta name="og:title" content="Paper Presentation(1)-ImageNet CLassification with Deep Convolutional Neural Networks">
      <meta name="og:description"
          content="Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton">
      
  </head>  

  <body>
      <div class="container">
          <header>
              <div class="row address-horizontal">
                  <div class="col-md-4 pull-left">
                      <address>
                          <strong>Andy_Lee</strong>
                          <br/>
                          <a href="mailto:690312856@qq.com">690312856@qq.com</a>
                      </address>
                  </div>
                  <div class="col-md-6 pull-right address-right">
                      <address>
                          <a href="http://www.scut.edu.cn/sse">School of Software Engineering</a>
                          <br/>
                          <a href="http://www.scut.edu.cn">South China University of Technology</a>
                      </address>  
                  </div>
              </div>
              <div class="row address-vertical">
                  <div class="col-md-12">
                      <address>
                          <strong>Andy_Lee</strong>
                          <br/>
                          <a href="mailto:690312856@qq.com">690312856@qq.com</a>
                          <br/>
                          <a href="http://www.scut.edu.cn/sse">School of Software Engineering</a>
                          <br/>
                          <a href="http://www.scut.edu.cn">South China University of Technology</a>
                      </address>  
                  </div>
              </div>

              <nav class="navbar navbar-default" role="navigation">
                  <div class="container-fluid">
                      <div class="navbar-header">
                          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar-collapse">
                          <span class="navbar-text">
                              Menu
                          </span>
                          <span class="caret"></span>
                          </button>
                      </div>

                      <div class="collapse navbar-collapse" id="navbar-collapse">
                          <ul class="nav navbar-nav">
                              
                                  <li><a href="/"> Home </a></li>
                              
                                  <li><a href="/blog"> Blog </a></li>
                              
                                  <li><a href="/projects"> Projects </a></li>
                              
                                  <li><a href="/publications.html"> Publications </a></li>
                              
                                  <li><a href="/cv.html"> CV </a></li>
                              
                          </ul>
                      </div>
                  </div>
              </nav>  
          </header>
          <h1>Paper Presentation(1)-ImageNet CLassification with Deep Convolutional Neural Networks</h1>

<p>Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton</p>

<p>Download paper:<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">here</a></p>

<h4 id="abstract">Abstract：</h4>

<blockquote>
<p><font size=1>We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif- ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train- ing faster, we used non-saturating neurons and a very efficient GPU implemen- tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.</font></p>
</blockquote>

<p>Last semester,I took part in a competition hold by <a href="http://www.wid.org.cn/project/2015ccf/index.php"><em>CCF</em></a> about clothing images classification. The dataset offered by <a href="https://www.jd.com/">JD.com</a>. It&#39;s the first time to solve such a difficult problem I have never faced. I remember very clearly that I came to a deadlock for several weeks. During that days,I found many many methods and read lots of papers. I didn&#39;t get the effective way until I read this paper. Due to lack of GPU,I trained the model by CPU for 5 days,the accuracy is on the verge of 56%. In the end,it&#39;s too late to commit the result and it made me upset.</p>

<p>The most gratifying thing is I am interested in the deep learning.So I began to learn the CNNs and deep learning...I have learned the deep learning for half a year up to now.Of cource,I DIYed a workstation included GPU.The deep learning thought give me a lot of thinking and bewilderment.From some experiments of my own,I got better results than I expected.</p>

<p>It‘s the first paper I read to learn the knowledge about CNNs.It leaded me to enter the door of deep learning.The paper published in NIPS2012,Hinton and his students achieved top-1 and top-5 error rates of 37.5% and 17.0% on the test data which is considerably better than the previous state-of-the-art in the ImageNet LSVRC-2010 contest and this article showed their methods.</p>

<h4 id="objective">Objective</h4>

<p>We trained a deep convolutional neural network,which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax.</p>

<h4 id="dataset">Dataset</h4>

<p><a href="http://www.image-net.org/">ImageNet</a> is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories.ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories.In all,there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.</p>

<h4 id="the-architecture">The Architecture</h4>

<p>The architecture of our network is summarized below.The net contains eight layers with weights.The first five are convolutional and the remaining three are fully-connected.The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel maps in the previous layer which reside on the same GPU. The kernels of the third convolutional layer are connected to all kernel maps in the second layer. The neurons in the fully-connected layers are connected to all neurons in the previous layer. Response-normalization layers follow the first and second convolutional layers. Max-pooling layers,follow both response-normalization layers as well as the fifth convolutional layer. The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer.</p>

<p>The first convolutional layer filters the 224 × 224 × 3 input image with 96 kernels of size 11 × 11 × 3 with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring neurons in a kernel map). The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 × 5 × 48. The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers. The third convolutional layer has 384 kernels of size 3 × 3 × 256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth convolutional layer has 384 kernels of size 3 × 3 × 192 , and the fifth convolutional layer has 256 kernels of size 3 × 3 × 192. The fully-connected layers have 4096 neurons each.</p>

<p><img class="post-image" src="/cnn.jpeg" /></p>

<h6 id="1-relu-nonlinearity">1. RelU Nonlinearity</h6>

<p>Why do we use ReLU activation function?</p>

<ul>
<li>If use sigmoid or tanh function(saturating function),cause large calculation（exponential operation，derivative solution）.</li>
<li>If use sigmoid or tanh function,the disadvantage is gradient disappearance,the derivative will trend to be 0.</li>
<li>Some output will be 0,the network becomes sparse,it is good to reduce to overfitting.</li>
</ul>

<h6 id="2-training-on-multiple-gpus">2. Training on Multiple GPUs</h6>

<p>A single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks that can be trained on it. It turns out that 1.2 million training examples are enough to train networks which are too big to fit on one GPU. Therefore we spread the net across two GPUs.The GPUs communicate only in certain layers. Such,we found the performance and accurate balance in the balance between connection and non connection.</p>

<h6 id="3-local-response-normalization">3. Local Response Normalization</h6>

<p>We find that local normalization scheme aids generalization.We apply local response normalization in the response of the same layer to the adjacent nodes.</p>

<h6 id="4-overlapping-pooling">4. Overlapping Pooling</h6>

<ul>
<li>Invariance：pay more attention to the existence of features rather than the position,the network will more tolerate some variance.</li>
<li>reduce the data quantity.</li>
</ul>

<h4 id="reducing-overfitting">Reducing Overfitting</h4>

<h6 id="1-data-augmentation">1. Data Augmentation</h6>

<p>The first form of data augmentation consists of generating image translations and horizontal reflec- tions. We do this by extracting random 224 × 224 patches (and their horizontal reflections) from the 256×256 images and training our network on these extracted patches4. This increases the size of our training set by a factor of 2048, though the resulting training examples are, of course, highly inter- dependent.</p>

<p>The second form of data augmentation consists of altering the intensities of the RGB channels in training images. Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNet training set. </p>

<h6 id="2-dropout">2. Dropout</h6>

<p>The technique, called “dropout”, consists of setting to zero the output of each hidden neuron with probability 0.5. The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in back- propagation. So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.</p>

<h4 id="details-of-learning">Details of learning</h4>

<p>We trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005. We found that this small amount of weight decay was important for the model to learn. In other words, weight decay here is not merely a regularizer: it reduces the model’s training error.</p>

<p>We initialized the weights in each layer from a zero-mean Gaussian distribution with standard de- viation 0.01. We initialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates the early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron biases in the remaining layers with the constant 0.</p>

<p>We used an equal learning rate for all layers, which we adjusted manually throughout training. The heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and reduced three times prior to termination.</p>

<h4 id="results">Results</h4>

<table><thead>
<tr>
<th>Model</th>
<th>Top-1</th>
<th>Top-5</th>
</tr>
</thead><tbody>
<tr>
<td>Sparse coding</td>
<td>47.1%</td>
<td>28.2%</td>
</tr>
<tr>
<td>SIFT + FVs</td>
<td>45.7%</td>
<td>25.7%</td>
</tr>
<tr>
<td>CNN</td>
<td>37.5%</td>
<td>17.0%</td>
</tr>
</tbody></table>

<br/>
<a href="/blog">Back to blog</a>
      </div>

      <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
      <script src="/js/bootstrap.min.js"></script>
      <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=Tex-AMS-MML_HTMLorMML"></script>
      
  </body>

</html>
