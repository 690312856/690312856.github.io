<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Andy_Lee&#39;s home page</title>
    <description></description>
    <link>http://yourdomain.com/</link>
    <atom:link href="http://yourdomain.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 01 Mar 2016 15:52:08 +0800</pubDate>
    <lastBuildDate>Tue, 01 Mar 2016 15:52:08 +0800</lastBuildDate>
    <generator>Jekyll v3.1.2</generator>
    
      <item>
        <title>Welcome to Jekyll!</title>
        <description>&lt;p&gt;You’ll find this post in your &lt;code&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;def print_hi(name)
  puts &amp;quot;Hi, #{name}&amp;quot;
end
print_hi(&amp;#39;Tom&amp;#39;)
#=&amp;gt; prints &amp;#39;Hi, Tom&amp;#39; to STDOUT.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;http://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Mon, 29 Feb 2016 21:30:36 +0800</pubDate>
        <link>http://yourdomain.com/jekyll/update/welcome-to-jekyll</link>
        <guid isPermaLink="true">http://yourdomain.com/jekyll/update/welcome-to-jekyll</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>hfhfhhf</title>
        <description>&lt;p&gt;After months of rumination, I have finally converged to a good personal website solution.
As it turned out, the toughest part was to choose the right frameworks!&lt;/p&gt;
</description>
        <pubDate>Sun, 24 Jan 2016 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/blog/hfhfhhf%E5%89%AF%E6%9C%AC</link>
        <guid isPermaLink="true">http://yourdomain.com/blog/hfhfhhf%E5%89%AF%E6%9C%AC</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>hfhfhhf</title>
        <description>&lt;p&gt;After months of rumination, I have finally converged to a good personal website solution.
As it turned out, the toughest part was to choose the right frameworks!&lt;/p&gt;
</description>
        <pubDate>Sun, 24 Jan 2016 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/blog/hfhfhhf</link>
        <guid isPermaLink="true">http://yourdomain.com/blog/hfhfhhf</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Introducing CUDArray</title>
        <description>&lt;p&gt;Neural networks and deep learning are booming (still).
Quite a few software frameworks have appeared over the last year, though none that allow high-level Python/NumPy programming with fast underlying array operations.
In this post, I present my attempt at making the two ends meet.
&lt;a href=&quot;http://github.com/andersbll/cudarray&quot;&gt;CUDArray&lt;/a&gt; is a CUDA-accelerated subset of the NumPy library with support for neural networks as its primary goal.&lt;/p&gt;

&lt;p&gt;A couple of weeks ago, I tried implementing a handful of NumPy functions using CUDA which turned out to be pretty fun (I just had to fill in the blanks for NumPy&amp;#39;s fine interface).
With my appetite whetted, I started developing CUDArray and a deep learning library, &lt;a href=&quot;http://github.com/andersbll/deeppy&quot;&gt;deeppy&lt;/a&gt; on top of it.
I plan on elaborating on deeppy in another post - in the meantime, I encourage you to look at the &lt;a href=&quot;http://github.com/andersbll/deeppy/tree/master/examples&quot;&gt;examples&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The behavior of CUDArray resembles that of &lt;a href=&quot;http://github.com/cudamat/cudamat&quot;&gt;CUDAMat&lt;/a&gt;/&lt;a href=&quot;http://www.cs.toronto.edu/%7Etijmen/gnumpy.html&quot;&gt;Gnumpy&lt;/a&gt; to a large degree and you might ask why I didn&amp;#39;t just build on top of these libraries.
Most notably, CUDAMat uses &lt;a href=&quot;https://docs.python.org/3/library/ctypes.html&quot;&gt;ctypes&lt;/a&gt; whereas CUDArray uses &lt;a href=&quot;http://cython.org/&quot;&gt;Cython&lt;/a&gt; to wrap C/C++ code.&lt;/p&gt;

&lt;p&gt;I should warn you that CUDArray is still in its infancy and that there is a lot of work to be done to mature and extend the framework.
However, I find CUDArray/deepy a very promising approach to deep learning considering the few weeks it took to code.&lt;/p&gt;

&lt;p&gt;Have a look at the &lt;a href=&quot;http://github.com/andersbll/cudarray&quot;&gt;source&lt;/a&gt; and be sure to check out the &lt;a href=&quot;/pubs/larsen2014cudarray.pdf&quot;&gt;technical report&lt;/a&gt; since it&amp;#39;s the only documentation I have written so far!&lt;/p&gt;
</description>
        <pubDate>Sat, 08 Nov 2014 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/blog/cudarray%E5%89%AF%E6%9C%AC</link>
        <guid isPermaLink="true">http://yourdomain.com/blog/cudarray%E5%89%AF%E6%9C%AC</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Introducing CUDArray</title>
        <description>&lt;p&gt;Neural networks and deep learning are booming (still).
Quite a few software frameworks have appeared over the last year, though none that allow high-level Python/NumPy programming with fast underlying array operations.
In this post, I present my attempt at making the two ends meet.
&lt;a href=&quot;http://github.com/andersbll/cudarray&quot;&gt;CUDArray&lt;/a&gt; is a CUDA-accelerated subset of the NumPy library with support for neural networks as its primary goal.&lt;/p&gt;

&lt;p&gt;A couple of weeks ago, I tried implementing a handful of NumPy functions using CUDA which turned out to be pretty fun (I just had to fill in the blanks for NumPy&amp;#39;s fine interface).
With my appetite whetted, I started developing CUDArray and a deep learning library, &lt;a href=&quot;http://github.com/andersbll/deeppy&quot;&gt;deeppy&lt;/a&gt; on top of it.
I plan on elaborating on deeppy in another post - in the meantime, I encourage you to look at the &lt;a href=&quot;http://github.com/andersbll/deeppy/tree/master/examples&quot;&gt;examples&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The behavior of CUDArray resembles that of &lt;a href=&quot;http://github.com/cudamat/cudamat&quot;&gt;CUDAMat&lt;/a&gt;/&lt;a href=&quot;http://www.cs.toronto.edu/%7Etijmen/gnumpy.html&quot;&gt;Gnumpy&lt;/a&gt; to a large degree and you might ask why I didn&amp;#39;t just build on top of these libraries.
Most notably, CUDAMat uses &lt;a href=&quot;https://docs.python.org/3/library/ctypes.html&quot;&gt;ctypes&lt;/a&gt; whereas CUDArray uses &lt;a href=&quot;http://cython.org/&quot;&gt;Cython&lt;/a&gt; to wrap C/C++ code.&lt;/p&gt;

&lt;p&gt;I should warn you that CUDArray is still in its infancy and that there is a lot of work to be done to mature and extend the framework.
However, I find CUDArray/deepy a very promising approach to deep learning considering the few weeks it took to code.&lt;/p&gt;

&lt;p&gt;Have a look at the &lt;a href=&quot;http://github.com/andersbll/cudarray&quot;&gt;source&lt;/a&gt; and be sure to check out the &lt;a href=&quot;/pubs/larsen2014cudarray.pdf&quot;&gt;technical report&lt;/a&gt; since it&amp;#39;s the only documentation I have written so far!&lt;/p&gt;
</description>
        <pubDate>Sat, 08 Nov 2014 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/blog/cudarray</link>
        <guid isPermaLink="true">http://yourdomain.com/blog/cudarray</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>FFT-based convolutional neural networks</title>
        <description>&lt;p&gt;A few weeks ago, I decided to implement my own convolution operations for the GPU.
My motivation was the need for an implementation that could be easily modified.
Unfortunately, most implementations available online are either slow or a big mess code-wise:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://code.google.com/p/cuda-convnet/&quot;&gt;cuda_convnet&lt;/a&gt;: Very fast thanks to the highly tuned CUDA code. The convolutions functions alone (&lt;a href=&quot;http://code.google.com/p/cuda-convnet/source/browse/trunk/src/cudaconv2/weight_acts.cu&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;http://code.google.com/p/cuda-convnet/source/browse/trunk/src/cudaconv2/filter_acts.cu&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;http://code.google.com/p/cuda-convnet/source/browse/trunk/src/cudaconv2/img_acts.cu&quot;&gt;3&lt;/a&gt;) are several 1,000 lines of code. Impossible to edit for anyone besides the original author.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.deeplearning.net/software/theano/&quot;&gt;Theano&lt;/a&gt;: Slower than cuda_convnet, still a bit messy using shared memory and other CUDA tricks.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://caffe.berkeleyvision.org/&quot;&gt;Caffe&lt;/a&gt;: In my experience a factor of 3 slower than cuda_convnet (the authors state otherwise). Their implementation is nice and simple consisting of &lt;a href=&quot;http://www.mathworks.se/help/images/ref/im2col.html&quot;&gt;im2col&lt;/a&gt; operations and cuBLAS matrix multiplications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I played around with all three above and even tried to do my own vanilla CUDA implementation (Big mistake! The performance was ~8 times slower than cuda_convnet).&lt;/p&gt;

&lt;p&gt;I then discovered the paper &lt;a href=&quot;http://arxiv.org/abs/1312.5851&quot;&gt;Fast Training of Convolutional Networks through FFTs&lt;/a&gt;, which was quite an interesting read (if only I had found it earlier!).
FFT-based convolutions had crossed my mind before, but I suspected the filter sizes were too small for the convolutions to be worthwhile in Fourier domain. 
As it turns out, FFT-based convolutions are quite competitive; mainly for the following reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Fourier transformations of filters can be reused as the filters are convolved with multiple images in a mini-batch.&lt;/li&gt;
&lt;li&gt;The Fourier transformations of the output gradients can be reused when back propagating gradients to both filters and input images.&lt;/li&gt;
&lt;li&gt;Summation over input channels can be performed in the Fourier domain, such that inverse Fourier transformations are only required once per output channel per image.&lt;/li&gt;
&lt;li&gt;Efficient, batched FFT implementation are available using &lt;a href=&quot;http://docs.nvidia.com/cuda/pdf/CUFFT_Library.pdf&quot;&gt;cuFFT&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The point-wise convolutions can be implemented as batched matrix multiplications using &lt;a href=&quot;http://docs.nvidia.com/cuda/pdf/CUBLAS_Library.pdf&quot;&gt;cuBLAS&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I also discovered that Sander Dieleman had experimented with &lt;a href=&quot;http://benanne.github.io/2014/05/12/fft-convolutions-in-theano.html&quot;&gt;FFT convolutions for Theano&lt;/a&gt;.
Unfortunately, his implementation does not currently include back propagation of gradients.
Moreover, it is written in high-level Theano, which I suspect is not flexible enough for an efficient implementation.&lt;/p&gt;

&lt;h2 id=&quot;my-implementation&quot;&gt;My implementation&lt;/h2&gt;

&lt;p&gt;After the above failed attempts at doing my own convolutions, the FFT approach was a refreshing angle.
It took some time to figure out the functioning of the &lt;a href=&quot;http://docs.nvidia.com/cuda/cufft/#function-cufftplanmany&quot;&gt;batched&lt;/a&gt; cuFFT operations with &lt;a href=&quot;http://docs.nvidia.com/cuda/cufft/#advanced-data-layout&quot;&gt;advanced data layout&lt;/a&gt; (which, btw., I&amp;#39;d prefer any day over fiddling with indexing errors in ordinary convolutions!).&lt;/p&gt;

&lt;p&gt;I now have a working implementation with the following highlights:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Supports back-propagation of gradients for both input images and filters.&lt;/li&gt;
&lt;li&gt;Supports 0-padding of image borders.&lt;/li&gt;
&lt;li&gt;No limitations on filter sizes / number of channels. Though, one should aim for image dimensions that are powers of 2, 3, 5, or 7 for faster cuFFT operations.&lt;/li&gt;
&lt;li&gt;Contains (almost) no GPU architecture specific fine-tuning. This is taken care of by cuBLAS/cuFFT.&lt;/li&gt;
&lt;li&gt;Relatively simple implementation (under &lt;a href=&quot;http://github.com/andersbll/theano_ops/blob/master/theano_ops/abll/src/abll/conv_bc01_fft.cu&quot;&gt;350 lines&lt;/a&gt; at the time of writing).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The implementation is still WIP but looks promising in terms of speed.
It even comes with a crude Theano wrapper.
Benchmarks will follow as soon as the Theano integration is done.
I have yet to figure out how to properly handle buffers and reusing FFTs in back propagation functions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://github.com/andersbll/theano_ops&quot;&gt;Check it out today!&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 14 Jul 2014 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/blog/fft_based_cnn%E5%89%AF%E6%9C%AC</link>
        <guid isPermaLink="true">http://yourdomain.com/blog/fft_based_cnn%E5%89%AF%E6%9C%AC</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>FFT-based convolutional neural networks</title>
        <description>&lt;p&gt;A few weeks ago, I decided to implement my own convolution operations for the GPU.
My motivation was the need for an implementation that could be easily modified.
Unfortunately, most implementations available online are either slow or a big mess code-wise:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://code.google.com/p/cuda-convnet/&quot;&gt;cuda_convnet&lt;/a&gt;: Very fast thanks to the highly tuned CUDA code. The convolutions functions alone (&lt;a href=&quot;http://code.google.com/p/cuda-convnet/source/browse/trunk/src/cudaconv2/weight_acts.cu&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;http://code.google.com/p/cuda-convnet/source/browse/trunk/src/cudaconv2/filter_acts.cu&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;http://code.google.com/p/cuda-convnet/source/browse/trunk/src/cudaconv2/img_acts.cu&quot;&gt;3&lt;/a&gt;) are several 1,000 lines of code. Impossible to edit for anyone besides the original author.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.deeplearning.net/software/theano/&quot;&gt;Theano&lt;/a&gt;: Slower than cuda_convnet, still a bit messy using shared memory and other CUDA tricks.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://caffe.berkeleyvision.org/&quot;&gt;Caffe&lt;/a&gt;: In my experience a factor of 3 slower than cuda_convnet (the authors state otherwise). Their implementation is nice and simple consisting of &lt;a href=&quot;http://www.mathworks.se/help/images/ref/im2col.html&quot;&gt;im2col&lt;/a&gt; operations and cuBLAS matrix multiplications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I played around with all three above and even tried to do my own vanilla CUDA implementation (Big mistake! The performance was ~8 times slower than cuda_convnet).&lt;/p&gt;

&lt;p&gt;I then discovered the paper &lt;a href=&quot;http://arxiv.org/abs/1312.5851&quot;&gt;Fast Training of Convolutional Networks through FFTs&lt;/a&gt;, which was quite an interesting read (if only I had found it earlier!).
FFT-based convolutions had crossed my mind before, but I suspected the filter sizes were too small for the convolutions to be worthwhile in Fourier domain. 
As it turns out, FFT-based convolutions are quite competitive; mainly for the following reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Fourier transformations of filters can be reused as the filters are convolved with multiple images in a mini-batch.&lt;/li&gt;
&lt;li&gt;The Fourier transformations of the output gradients can be reused when back propagating gradients to both filters and input images.&lt;/li&gt;
&lt;li&gt;Summation over input channels can be performed in the Fourier domain, such that inverse Fourier transformations are only required once per output channel per image.&lt;/li&gt;
&lt;li&gt;Efficient, batched FFT implementation are available using &lt;a href=&quot;http://docs.nvidia.com/cuda/pdf/CUFFT_Library.pdf&quot;&gt;cuFFT&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The point-wise convolutions can be implemented as batched matrix multiplications using &lt;a href=&quot;http://docs.nvidia.com/cuda/pdf/CUBLAS_Library.pdf&quot;&gt;cuBLAS&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I also discovered that Sander Dieleman had experimented with &lt;a href=&quot;http://benanne.github.io/2014/05/12/fft-convolutions-in-theano.html&quot;&gt;FFT convolutions for Theano&lt;/a&gt;.
Unfortunately, his implementation does not currently include back propagation of gradients.
Moreover, it is written in high-level Theano, which I suspect is not flexible enough for an efficient implementation.&lt;/p&gt;

&lt;h2 id=&quot;my-implementation&quot;&gt;My implementation&lt;/h2&gt;

&lt;p&gt;After the above failed attempts at doing my own convolutions, the FFT approach was a refreshing angle.
It took some time to figure out the functioning of the &lt;a href=&quot;http://docs.nvidia.com/cuda/cufft/#function-cufftplanmany&quot;&gt;batched&lt;/a&gt; cuFFT operations with &lt;a href=&quot;http://docs.nvidia.com/cuda/cufft/#advanced-data-layout&quot;&gt;advanced data layout&lt;/a&gt; (which, btw., I&amp;#39;d prefer any day over fiddling with indexing errors in ordinary convolutions!).&lt;/p&gt;

&lt;p&gt;I now have a working implementation with the following highlights:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Supports back-propagation of gradients for both input images and filters.&lt;/li&gt;
&lt;li&gt;Supports 0-padding of image borders.&lt;/li&gt;
&lt;li&gt;No limitations on filter sizes / number of channels. Though, one should aim for image dimensions that are powers of 2, 3, 5, or 7 for faster cuFFT operations.&lt;/li&gt;
&lt;li&gt;Contains (almost) no GPU architecture specific fine-tuning. This is taken care of by cuBLAS/cuFFT.&lt;/li&gt;
&lt;li&gt;Relatively simple implementation (under &lt;a href=&quot;http://github.com/andersbll/theano_ops/blob/master/theano_ops/abll/src/abll/conv_bc01_fft.cu&quot;&gt;350 lines&lt;/a&gt; at the time of writing).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The implementation is still WIP but looks promising in terms of speed.
It even comes with a crude Theano wrapper.
Benchmarks will follow as soon as the Theano integration is done.
I have yet to figure out how to properly handle buffers and reusing FFTs in back propagation functions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://github.com/andersbll/theano_ops&quot;&gt;Check it out today!&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 14 Jul 2014 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/blog/fft_based_cnn</link>
        <guid isPermaLink="true">http://yourdomain.com/blog/fft_based_cnn</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>A simple implementation of convolutional neural networks</title>
        <description>&lt;p&gt;I was recently asked for a simple implementation of a convolutional neural network (CNN).
The purpose was to allow GPU-savvy programmers to understand the problem by inspecting the code; and to serve as reference for their optimized implementation.
This request reignited a frustration from when I myself started looking into CNNs a couple of months ago: There are no easily read implementations available!
Most CNN implementations are either highly optimized GPU code or contain only barebone operations in a non-modular code structure.
In either case, the code is hard to read and the back-propagation algorithm is difficult to recognize.&lt;/p&gt;

&lt;p&gt;As I failed to find anything usable online and, more likely, because I&amp;#39;m a computer scientist at heart, I ended up coding my own toy CNN from scratch! 
The top priority was simplicity - so Python/NumPy was a given.
For the performance critical operations (convolution and max-pooling), I had to use Cython to get tolerable speed.
The implementation is &lt;a href=&quot;https://github.com/andersbll/nnet&quot;&gt;available on my Github&lt;/a&gt;.
I have even included some &lt;a href=&quot;https://github.com/andersbll/nnet/blob/master/examples&quot;&gt;usage examples&lt;/a&gt; that should work right out of the box - just a &lt;code&gt;git clone&lt;/code&gt; away!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; My CNN implementation is not in any way competitive with more mature libraries in terms of features and speed.
The code is only meant as a readable example of feed-forward neural networks.&lt;/p&gt;

&lt;h2 id=&quot;implementation-tips&quot;&gt;Implementation tips&lt;/h2&gt;

&lt;p&gt;Here is a list of lessons, of which some were learned in a pretty time-consumingly way.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Check your back-propagated gradients for correctness using finite-difference calculation! You might as well think this into your program design from the start as this is the only sane way to verify the correctness of your implementation. And yes, bugs are inevitable.&lt;/li&gt;
&lt;li&gt;Make the network modular at the layer-level. This is not new if you have studied other popular implementations like &lt;a href=&quot;http://code.google.com/p/cuda-convnet/&quot;&gt;cuda-convnet&lt;/a&gt; or &lt;a href=&quot;http://eblearn.sourceforge.net/&quot;&gt;EBLearn&lt;/a&gt;.
Each layer should then implement a method for forward propagating the input and a method for back propagating the gradients. &lt;/li&gt;
&lt;li&gt;Standard library &lt;a href=&quot;http://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.signal.convolve2d.html&quot;&gt;convolution operations&lt;/a&gt; are not suitable for CNNs. Typically, you work on a batch of images per gradient update.
Each image in this batch contains multiple channels and you convolve each image with a 3D filter to get one output channel.
If you perform 3D convolution, you would be restricted to a &amp;#39;valid&amp;#39; convolution because you should not move you along the channel axis.
If you perform 2D convolution you can perform &amp;#39;valid&amp;#39;, &amp;#39;same&amp;#39; or &amp;#39;full&amp;#39; convolutions as you wish, however, you must perform many separate convolutions which is not good in terms of efficiency.
Moreover, I have yet to figure out how one would calculate the gradients of the weights from standard convolution operations.&lt;/li&gt;
&lt;li&gt;If you reach the edge of what NumPy is good for and it starts getting complicated; use Cython!
I spent a lot of time implementing max-pooling with &lt;a href=&quot;http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.strides.html&quot;&gt;striding tricks&lt;/a&gt; to allow for sliding windows.
It was a mess in terms of readability.
In comparison, &lt;a href=&quot;https://github.com/andersbll/nnet/blob/master/nnet/convnet/pool.pyx&quot;&gt;a couple of nested for loops&lt;/a&gt; in Cython are both easier to read and faster.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 22 May 2014 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/blog/simple_cnn%E5%89%AF%E6%9C%AC</link>
        <guid isPermaLink="true">http://yourdomain.com/blog/simple_cnn%E5%89%AF%E6%9C%AC</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>A simple implementation of convolutional neural networks</title>
        <description>&lt;p&gt;I was recently asked for a simple implementation of a convolutional neural network (CNN).
The purpose was to allow GPU-savvy programmers to understand the problem by inspecting the code; and to serve as reference for their optimized implementation.
This request reignited a frustration from when I myself started looking into CNNs a couple of months ago: There are no easily read implementations available!
Most CNN implementations are either highly optimized GPU code or contain only barebone operations in a non-modular code structure.
In either case, the code is hard to read and the back-propagation algorithm is difficult to recognize.&lt;/p&gt;

&lt;p&gt;As I failed to find anything usable online and, more likely, because I&amp;#39;m a computer scientist at heart, I ended up coding my own toy CNN from scratch! 
The top priority was simplicity - so Python/NumPy was a given.
For the performance critical operations (convolution and max-pooling), I had to use Cython to get tolerable speed.
The implementation is &lt;a href=&quot;https://github.com/andersbll/nnet&quot;&gt;available on my Github&lt;/a&gt;.
I have even included some &lt;a href=&quot;https://github.com/andersbll/nnet/blob/master/examples&quot;&gt;usage examples&lt;/a&gt; that should work right out of the box - just a &lt;code&gt;git clone&lt;/code&gt; away!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; My CNN implementation is not in any way competitive with more mature libraries in terms of features and speed.
The code is only meant as a readable example of feed-forward neural networks.&lt;/p&gt;

&lt;h2 id=&quot;implementation-tips&quot;&gt;Implementation tips&lt;/h2&gt;

&lt;p&gt;Here is a list of lessons, of which some were learned in a pretty time-consumingly way.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Check your back-propagated gradients for correctness using finite-difference calculation! You might as well think this into your program design from the start as this is the only sane way to verify the correctness of your implementation. And yes, bugs are inevitable.&lt;/li&gt;
&lt;li&gt;Make the network modular at the layer-level. This is not new if you have studied other popular implementations like &lt;a href=&quot;http://code.google.com/p/cuda-convnet/&quot;&gt;cuda-convnet&lt;/a&gt; or &lt;a href=&quot;http://eblearn.sourceforge.net/&quot;&gt;EBLearn&lt;/a&gt;.
Each layer should then implement a method for forward propagating the input and a method for back propagating the gradients. &lt;/li&gt;
&lt;li&gt;Standard library &lt;a href=&quot;http://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.signal.convolve2d.html&quot;&gt;convolution operations&lt;/a&gt; are not suitable for CNNs. Typically, you work on a batch of images per gradient update.
Each image in this batch contains multiple channels and you convolve each image with a 3D filter to get one output channel.
If you perform 3D convolution, you would be restricted to a &amp;#39;valid&amp;#39; convolution because you should not move you along the channel axis.
If you perform 2D convolution you can perform &amp;#39;valid&amp;#39;, &amp;#39;same&amp;#39; or &amp;#39;full&amp;#39; convolutions as you wish, however, you must perform many separate convolutions which is not good in terms of efficiency.
Moreover, I have yet to figure out how one would calculate the gradients of the weights from standard convolution operations.&lt;/li&gt;
&lt;li&gt;If you reach the edge of what NumPy is good for and it starts getting complicated; use Cython!
I spent a lot of time implementing max-pooling with &lt;a href=&quot;http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.strides.html&quot;&gt;striding tricks&lt;/a&gt; to allow for sliding windows.
It was a mess in terms of readability.
In comparison, &lt;a href=&quot;https://github.com/andersbll/nnet/blob/master/nnet/convnet/pool.pyx&quot;&gt;a couple of nested for loops&lt;/a&gt; in Cython are both easier to read and faster.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 22 May 2014 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/blog/simple_cnn</link>
        <guid isPermaLink="true">http://yourdomain.com/blog/simple_cnn</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Hello brave new world</title>
        <description>&lt;p&gt;After months of rumination, I have finally converged to a good personal website solution.
As it turned out, the toughest part was to choose the right frameworks!
At first, I was reluctant to use the popular &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt; (I&amp;#39;m not a Ruby programmer) in favor of the Python-based &lt;a href=&quot;http://docs.getpelican.com/&quot;&gt;Pelican&lt;/a&gt;.
However, Jekyll and its surrounding ecosystem is more mature and I have not run into limitations like I did with my initial attempt using Pelican.&lt;/p&gt;

&lt;p&gt;Some highlights of this website are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Static site generation.
This allows me to edit plain text files with &lt;a href=&quot;http://en.wikipedia.org/wiki/Markdown&quot;&gt;Markdown syntax&lt;/a&gt; and having them transformed to static HTML that doesn&amp;#39;t require fancy server-side logic.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Responsive_web_design&quot;&gt;Responsive design&lt;/a&gt; using &lt;a href=&quot;http://getbootstrap.com/&quot;&gt;Bootstrap&lt;/a&gt;.
Bootstrap also provides a useful library of sane building blocks that allow a HTML/JavaScript illiterate like me to produce cross-platform websites.&lt;/li&gt;
&lt;li&gt;Good separation between layout and content.&lt;/li&gt;
&lt;li&gt;Easy integration with BibTeX using &lt;a href=&quot;http://github.com/inukshuk/jekyll-scholar&quot;&gt;Jekyll-Scholar&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;I can deploy changes and new content to the website with a simple &lt;code&gt;make deploy&lt;/code&gt; command.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The source of my website is &lt;a href=&quot;http://github.com/andersbll/website&quot;&gt;on Github&lt;/a&gt; - feel free to grab whatever you might find useful. Also, you should consider adding plenty of &lt;a href=&quot;http://html9responsiveboilerstrapjs.com/&quot;&gt;HTML9&lt;/a&gt; pizzazz!&lt;/p&gt;

&lt;h2 id=&quot;purpose-of-this-blog&quot;&gt;Purpose of this blog&lt;/h2&gt;

&lt;p&gt;I have included a blog on my website as an attempt to exercise my communication skills.
While academia is still focused on the traditional publication pipeline through peer-review, there are plenty of good web-based publication alternatives nowadays.
A blog is a practical way to publish work and findings in an informal manner.
Another motivation behind this blog is the chance to dump the academic by-products that would never get published otherwise (e.g. presentations, notes, tutorials, mediocre results).
I suspect these by-products can be put online without too much work - and they might even be useful for others.&lt;/p&gt;

&lt;p&gt;So far, this is all cheap talk on my part as I have yet to fill up this website!
My intentions are to write on a monthly basis (a lower bound).
At least, now that I have a website, I have one less excuse for keeping my work to myself.&lt;/p&gt;
</description>
        <pubDate>Fri, 25 Apr 2014 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/blog/hello_world%E5%89%AF%E6%9C%AC</link>
        <guid isPermaLink="true">http://yourdomain.com/blog/hello_world%E5%89%AF%E6%9C%AC</guid>
        
        
        <category>blog</category>
        
      </item>
    
  </channel>
</rss>
